{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688fc3d6",
   "metadata": {},
   "source": [
    "# Pre-Notebook Setup (automated)\n",
    "\n",
    "This cell guides setup and runs automated checks for reproducibility. You can run the helper scripts from a PowerShell environment (recommended):\n",
    "\n",
    "- To create / update a Conda environment using `environment.yml`:\n",
    "  ```powershell\n",
    "  ./setup_and_run.ps1 -CreateCondaEnv -EnvName bigdata\n",
    "  ```\n",
    "\n",
    "- To install pip requirements into the currently active Python:\n",
    "  ```powershell\n",
    "  ./setup_and_run.ps1 -InstallPipRequirements\n",
    "  ```\n",
    "\n",
    "- To run quick environment checks (dependency pins, pyspark smoke test) and the PySpark preprocessing step:\n",
    "  ```powershell\n",
    "  ./setup_and_run.ps1 -RunPySparkPreprocess\n",
    "  ```\n",
    "\n",
    "Note: When running the notebook from the Jupyter UI with a kernel that matches the environment created above, you may also execute the notebook using `papermill` with `-RunNotebook` to produce a reproducible executed notebook copy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5b28ce",
   "metadata": {},
   "source": [
    "# 00 - Data Preprocessing & EDA\n",
    "\n",
    "This notebook performs exploratory data analysis (EDA), creates reproducible preprocessing pipelines, precomputes embeddings (optional), engineers features, and saves processed artifacts for downstream modeling. It follows the agreed PRD data cleaning strategy and persists artifacts to `data/processed/v1/`, `models/pipelines/`, and `results/` for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f6ba73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HADOOP_HOME is set to: C:\\hadoop\n",
      "Setting PYSPARK_PYTHON to c:\\Users\\swkan\\Downloads\\VSCode\\Big Data Group Project\\.venv310\\Scripts\\python.exe\n",
      "Java version check passed.\n",
      "pyspark package version: 3.5.7\n",
      "Using existing active SparkSession: 3.5.7\n",
      "Spark is available; version: 3.5.7\n",
      "Current working directory: c:\\Users\\swkan\\Downloads\\VSCode\\Big Data Group Project\\notebooks\n",
      "Found dataset in ../data/ (adjusting expectations)\n",
      "Results dir exists: True\n"
     ]
    }
   ],
   "source": [
    "# Cell: Import libraries & setup\n",
    "import os, sys, json, logging, importlib, re, subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# --- Environment Fixes (Java & Spark) ---\n",
    "def fix_environment():\n",
    "    # 0. Clear potentially harmful global options that break Java 8 checks\n",
    "    if 'JAVA_TOOL_OPTIONS' in os.environ:\n",
    "        print(f\"Unsetting JAVA_TOOL_OPTIONS ({os.environ['JAVA_TOOL_OPTIONS']}) to avoid conflicts.\")\n",
    "        del os.environ['JAVA_TOOL_OPTIONS']\n",
    "\n",
    "    # 1. Unset SPARK_HOME to avoid conflicts with pip-installed pyspark.\n",
    "    #    However, we MUST set HADOOP_HOME on Windows for winutils.exe.\n",
    "    #    We also clear CLASSPATH to avoid loading conflicting JARs from other installations.\n",
    "    for env_var in ['SPARK_HOME', 'CLASSPATH', 'HADOOP_CLASSPATH', 'SPARK_DIST_CLASSPATH']:\n",
    "        if env_var in os.environ:\n",
    "            print(f\"Unsetting {env_var} ({os.environ[env_var]}) to avoid conflicts.\")\n",
    "            del os.environ[env_var]\n",
    "\n",
    "    # 1.1 Ensure HADOOP_HOME is set correctly for Windows (requires winutils.exe)\n",
    "    if 'HADOOP_HOME' not in os.environ:\n",
    "        # Try standard location\n",
    "        if os.path.exists(r'C:\\hadoop\\bin\\winutils.exe'):\n",
    "            os.environ['HADOOP_HOME'] = r'C:\\hadoop'\n",
    "            print(f\"Setting HADOOP_HOME to {os.environ['HADOOP_HOME']}\")\n",
    "        else:\n",
    "            print(\"Warning: HADOOP_HOME not set and C:\\\\hadoop\\\\bin\\\\winutils.exe not found. Spark file operations may fail on Windows.\")\n",
    "    else:\n",
    "        print(f\"HADOOP_HOME is set to: {os.environ['HADOOP_HOME']}\")\n",
    "    \n",
    "    # Add HADOOP_HOME/bin to PATH if not present\n",
    "    if 'HADOOP_HOME' in os.environ:\n",
    "        hadoop_bin = os.path.join(os.environ['HADOOP_HOME'], 'bin')\n",
    "        if hadoop_bin not in os.environ['PATH']:\n",
    "             os.environ['PATH'] = hadoop_bin + os.pathsep + os.environ['PATH']\n",
    "\n",
    "    # 1.5 Set PYSPARK_PYTHON to the current python executable to avoid \"python3\" not found errors on Windows\n",
    "    print(f\"Setting PYSPARK_PYTHON to {sys.executable}\")\n",
    "    os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "    # 2. Check Java Version and attempt to fix if Java 8 is detected\n",
    "    needs_fix = False\n",
    "    try:\n",
    "        # Check java on PATH\n",
    "        java_cmd = \"java\"\n",
    "        if \"JAVA_HOME\" in os.environ:\n",
    "            java_cmd = str(Path(os.environ[\"JAVA_HOME\"]) / \"bin\" / \"java\")\n",
    "\n",
    "        output = subprocess.check_output([java_cmd, \"-version\"], stderr=subprocess.STDOUT).decode()\n",
    "        if 'version \"1.8' in output or 'version \"8\"' in output:\n",
    "            print(\"Warning: Java 8 detected. Spark 3.5.x requires Java 17.\")\n",
    "            needs_fix = True\n",
    "        else:\n",
    "            print(\"Java version check passed.\")\n",
    "    except Exception as e:\n",
    "        print(\"Java check failed (likely due to env mismatch). Attempting to find Java 17...\", e)\n",
    "        needs_fix = True\n",
    "\n",
    "    if needs_fix:\n",
    "        # Attempt to find Java 17 in common Windows locations\n",
    "        candidates = []\n",
    "        for prog_files in [r\"C:\\Program Files\", r\"C:\\Program Files (x86)\"]:\n",
    "            p = Path(prog_files)\n",
    "            candidates.extend(p.glob(\"Eclipse Adoptium/jdk-17*\"))\n",
    "            candidates.extend(p.glob(\"Java/jdk-17*\"))\n",
    "            candidates.extend(p.glob(\"Zulu/zulu-17*\"))\n",
    "            candidates.extend(p.glob(\"Microsoft/jdk-17*\"))\n",
    "\n",
    "        if candidates:\n",
    "            best_java = candidates[0]\n",
    "            print(f\"Found Java 17 at: {best_java}\")\n",
    "            print(\"Setting JAVA_HOME and PATH for this session.\")\n",
    "            os.environ[\"JAVA_HOME\"] = str(best_java)\n",
    "            os.environ[\"PATH\"] = str(best_java / \"bin\") + os.pathsep + os.environ[\"PATH\"]\n",
    "        else:\n",
    "            print(\"CRITICAL: Java 17 not found. Please run `scripts/check_java_and_install_jdk.ps1` to install it.\")\n",
    "\n",
    "fix_environment()\n",
    "# ----------------------------------------\n",
    "\n",
    "# Spark imports (safe)\n",
    "try:\n",
    "    import pyspark\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.sql import types as T\n",
    "    print('pyspark package version:', pyspark.__version__)\n",
    "except Exception as e:\n",
    "    print('PySpark import warning:', e)\n",
    "\n",
    "# Setup logging for notebook\n",
    "LOG = logging.getLogger('notebook')\n",
    "if not LOG.handlers:\n",
    "    logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s', level=logging.INFO)\n",
    "\n",
    "# Ensure SparkSession exists (create a local one if needed) with robust fallbacks\n",
    "if 'spark' not in globals() or spark is None:\n",
    "    spark = None\n",
    "\n",
    "# 1. Try to get active session\n",
    "try:\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark:\n",
    "        print(\"Using existing active SparkSession:\", spark.version)\n",
    "except Exception:\n",
    "    spark = None\n",
    "\n",
    "# 2. Try builder.getOrCreate()\n",
    "if spark is None:\n",
    "    try:\n",
    "        # Add extra java options for Java 17+ module access if needed\n",
    "        spark = SparkSession.builder \\\n",
    "            .master('local[2]') \\\n",
    "            .appName('notebook-local') \\\n",
    "            .config(\"spark.driver.extraJavaOptions\", \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED\") \\\n",
    "            .config(\"spark.executor.extraJavaOptions\", \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED\") \\\n",
    "            .config(\"spark.pyspark.python\", sys.executable) \\\n",
    "            .config(\"spark.pyspark.driver.python\", sys.executable) \\\n",
    "            .getOrCreate()\n",
    "        print('Spark session started (local[2]):', spark.version)\n",
    "    except Exception as e:\n",
    "        print('Warning: Could not create SparkSession via builder:', e)\n",
    "        spark = None\n",
    "\n",
    "# 3. Fallback to existing SparkContext if present\n",
    "if spark is None:\n",
    "    try:\n",
    "        from pyspark import SparkContext\n",
    "        sc = SparkContext.getOrCreate()\n",
    "        # If there's an existing SparkContext, create SparkSession from it\n",
    "        try:\n",
    "            spark = SparkSession(sc)\n",
    "            print('Recovered existing SparkSession from SparkContext:', spark.version)\n",
    "        except Exception as e:\n",
    "            # In rare cases constructing SparkSession with the existing SC fails; still keep sc\n",
    "            print('Recovered SparkContext, but could not create SparkSession object; sc exists, version info might be in log:', e)\n",
    "            spark = None\n",
    "    except Exception as e:\n",
    "        print('No SparkContext found or could not recover:', e)\n",
    "        spark = None\n",
    "\n",
    "# 4. Final assertion and diagnostics\n",
    "if spark is None:\n",
    "    # Helpful diagnostics to guide the user\n",
    "    print('\\nFATAL: SparkSession could not be created or recovered in this notebook session.')\n",
    "    print('Suggestions:')\n",
    "    print(' - Restart the kernel and re-run the setup cell to ensure no stale contexts are present.')\n",
    "    print(' - Ensure you are not running another Spark process that holds a JVM context.')\n",
    "    print(' - Try running the helper scripts from a fresh shell with Java 17 and the same Python kernel activated:')\n",
    "    print('     Remove-Item Env:\\JAVA_TOOL_OPTIONS -ErrorAction SilentlyContinue; Remove-Item Env:\\SPARK_HOME -ErrorAction SilentlyContinue; $env:JAVA_HOME=\"C:\\\\Program Files\\\\Eclipse Adoptium\\\\jdk-17.0.16.8-hotspot\"; $env:PATH = $env:JAVA_HOME + \"\\\\bin;\" + $env:PATH; .\\\\.venv310\\\\Scripts\\\\Activate.ps1; python -c \"import pyspark; print(pyspark.__version__)\"')\n",
    "    raise RuntimeError('SparkSession creation failed; see diagnostics above.')\n",
    "\n",
    "# Basic environment diagnostic\n",
    "print('Spark is available; version:', spark.version)\n",
    "print('Current working directory:', Path.cwd())\n",
    "# Fix path check to look up one level if in notebooks dir\n",
    "data_path = Path('data/stock_market_crash_2022.csv')\n",
    "if not data_path.exists() and Path('../data/stock_market_crash_2022.csv').exists():\n",
    "    print('Found dataset in ../data/ (adjusting expectations)')\n",
    "else:\n",
    "    print('Found dataset in data dir:', data_path.exists())\n",
    "print('Results dir exists:', Path('results').exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80a47b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined and registered UDFs: clean_udf, feats_udf\n"
     ]
    }
   ],
   "source": [
    "# Cell: Define UDFs and Helper Functions\n",
    "# These functions are adapted from scripts/preprocess_pyspark.py to run interactively in the notebook.\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def clean_text_py(s: pd.Series) -> pd.Series:\n",
    "    URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    MENTION_RE = re.compile(r\"@\\w+\")\n",
    "    TICKER_RE = re.compile(r\"\\$[A-Za-z]+\")\n",
    "\n",
    "    def norm(x: str) -> str:\n",
    "        if pd.isna(x):\n",
    "            return \"\"\n",
    "        x = str(x)\n",
    "        try:\n",
    "            x = unicodedata.normalize(\"NFKC\", x)\n",
    "        except Exception:\n",
    "            pass\n",
    "        x = URL_RE.sub(\" <URL> \", x)\n",
    "        x = MENTION_RE.sub(\" <USER> \", x)\n",
    "        x = TICKER_RE.sub(\" <TICKER> \", x)\n",
    "        x = re.sub(r\"\\s+\", \" \", x)\n",
    "        return x.strip()\n",
    "\n",
    "    return s.apply(norm)\n",
    "\n",
    "def features_py(s: pd.Series) -> pd.DataFrame:\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def compute(row: str):\n",
    "        if pd.isna(row):\n",
    "            row = \"\"\n",
    "        # Use a safe length for caps ratio\n",
    "        chars = len(row)\n",
    "        char_count = chars\n",
    "        word_count = len(row.split())\n",
    "        hashtag_count = row.count('#')\n",
    "        mention_count = len(re.findall(r\"@\\w+\", row))\n",
    "        ticker_count = len(re.findall(r\"\\$[A-Za-z]+\", row))\n",
    "        caps_ratio = sum(1 for c in row if c.isupper()) / (chars + 1e-9)\n",
    "        emoji_count = sum(1 for c in row if ord(c) > 10000)\n",
    "        has_nonascii = any(ord(c) > 127 for c in row)\n",
    "        vader_scores = vader.polarity_scores(row)\n",
    "        return (\n",
    "            char_count,\n",
    "            word_count,\n",
    "            hashtag_count,\n",
    "            mention_count,\n",
    "            ticker_count,\n",
    "            caps_ratio,\n",
    "            emoji_count,\n",
    "            has_nonascii,\n",
    "            vader_scores.get(\"compound\", 0.0),\n",
    "            vader_scores.get(\"pos\", 0.0),\n",
    "            vader_scores.get(\"neu\", 0.0),\n",
    "            vader_scores.get(\"neg\", 0.0),\n",
    "        )\n",
    "\n",
    "    df = s.apply(compute)\n",
    "    # Return as a DataFrame with columns matching the struct fields\n",
    "    return pd.DataFrame(df.tolist(), columns=[\n",
    "        'char_count','word_count','hashtag_count','mention_count','ticker_count','caps_ratio',\n",
    "        'emoji_count','has_nonascii','vader_compound','vader_pos','vader_neu','vader_neg'\n",
    "    ])\n",
    "\n",
    "def features_schema():\n",
    "    return StructType([\n",
    "        StructField('char_count', IntegerType()),\n",
    "        StructField('word_count', IntegerType()),\n",
    "        StructField('hashtag_count', IntegerType()),\n",
    "        StructField('mention_count', IntegerType()),\n",
    "        StructField('ticker_count', IntegerType()),\n",
    "        StructField('caps_ratio', DoubleType()),\n",
    "        StructField('emoji_count', IntegerType()),\n",
    "        StructField('has_nonascii', BooleanType()),\n",
    "        StructField('vader_compound', DoubleType()),\n",
    "        StructField('vader_pos', DoubleType()),\n",
    "        StructField('vader_neu', DoubleType()),\n",
    "        StructField('vader_neg', DoubleType()),\n",
    "    ])\n",
    "\n",
    "# Register UDFs\n",
    "@pandas_udf(StringType())\n",
    "def clean_udf(s: pd.Series) -> pd.Series:\n",
    "    return clean_text_py(s)\n",
    "\n",
    "@pandas_udf(features_schema())\n",
    "def feats_udf(s: pd.Series) -> pd.DataFrame:\n",
    "    return features_py(s)\n",
    "\n",
    "print(\"Defined and registered UDFs: clean_udf, feats_udf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7777aa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java version info:\n",
      "openjdk version \"17.0.16\" 2025-07-15\n",
      "OpenJDK Runtime Environment Temurin-17.0.16+8 (build 17.0.16+8)\n",
      "OpenJDK 64-Bit Server VM Temurin-17.0.16+8 (build 17.0.16+8, mixed mode, sharing)\n",
      "\n",
      "Python version: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# Cell: Environment checks (Java & Python)\n",
    "import subprocess\n",
    "try:\n",
    "    java_out = subprocess.check_output(['java', '-version'], stderr=subprocess.STDOUT)\n",
    "    print('Java version info:')\n",
    "    print(java_out.decode())\n",
    "except Exception as e:\n",
    "    print('java -version unavailable or returned error:', e)\n",
    "\n",
    "print('Python version:', sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "271db47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: run_preprocess_spark - main preprocessing flow\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def run_preprocess_spark(csv_path: str, out_dir: str, seed: int = 42, test_size: float = 0.2):\n",
    "    \"\"\"Main preprocessing flow adapted from `scripts/preprocess_pyspark.py`.\n",
    "\n",
    "    This will:\n",
    "    - Read a CSV into Spark\n",
    "    - Normalize text and deduplicate\n",
    "    - Extract numeric & lexicon features via `feats_udf`\n",
    "    - Save label maps, counts, and stratified train/test parquets\n",
    "    \"\"\"\n",
    "    LOG.info(\"Reading CSV from %s\", csv_path)\n",
    "    df = spark.read.options(header=True, multiLine=True, escape='\"').csv(csv_path)\n",
    "\n",
    "    # ensure consistent column names\n",
    "    cols = df.columns\n",
    "    if 'text_sentiment' not in cols and 'label' in cols:\n",
    "        df = df.withColumnRenamed('label', 'text_sentiment')\n",
    "    if cols[0] != 'text':\n",
    "        df = df.withColumnRenamed(cols[0], 'text')\n",
    "\n",
    "    # Clean text using pandas_udf\n",
    "    df = df.withColumn('clean_text_sample', clean_udf(col('text')))\n",
    "\n",
    "    # Deduplicate\n",
    "    before = df.count()\n",
    "    df = df.dropDuplicates(['clean_text_sample'])\n",
    "    after = df.count()\n",
    "    LOG.info('Dropped %d duplicates', before - after)\n",
    "\n",
    "    # Extract features via padas_udf\n",
    "    df_feats = df.withColumn('features_struct', feats_udf(col('clean_text_sample')))\n",
    "    for f in features_schema().fieldNames():\n",
    "        df_feats = df_feats.withColumn(f, col('features_struct.' + f))\n",
    "    df = df_feats.drop('features_struct')\n",
    "\n",
    "    # Make sure label exists and is int\n",
    "    if 'label' not in df.columns and 'text_sentiment' in df.columns:\n",
    "        df = df.withColumn('label', col('text_sentiment').cast('int'))\n",
    "\n",
    "    # Save label_map to results\n",
    "    results_dir = Path('results')\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if 'label' in df.columns:\n",
    "        label_counts = df.groupBy('label').count().toPandas().set_index('label')['count'].to_dict()\n",
    "    elif 'text_sentiment' in df.columns:\n",
    "        label_counts = df.groupBy('text_sentiment').count().toPandas().set_index('text_sentiment')['count'].to_dict()\n",
    "    else:\n",
    "        label_counts = {}\n",
    "\n",
    "    with open(results_dir / 'label_map.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(label_counts, f, indent=2)\n",
    "\n",
    "    # combined_counts\n",
    "    count_col = 'label' if 'label' in df.columns else 'text_sentiment' if 'text_sentiment' in df.columns else None\n",
    "    if count_col is not None:\n",
    "        df.groupBy(count_col).count().orderBy(count_col).toPandas().to_csv(results_dir / 'combined_counts.csv', index=False)\n",
    "\n",
    "    # Stratified split using sampleBy on label\n",
    "    labels = [int(r['label']) for r in df.select('label').distinct().collect()]\n",
    "    fractions = {lab: test_size for lab in labels}\n",
    "    sample_df = df.stat.sampleBy('label', fractions, seed)\n",
    "\n",
    "    # train/test assignment using join on cleaned text value\n",
    "    test_df = df.join(sample_df.select('clean_text_sample'), on='clean_text_sample', how='inner')\n",
    "    train_df = df.join(sample_df.select('clean_text_sample'), on='clean_text_sample', how='left_anti')\n",
    "\n",
    "    # Save sample index map for reproducibility\n",
    "    idx_df = df.withColumn('_orig_index', monotonically_increasing_id())\n",
    "    if 'text_sentiment' in df.columns:\n",
    "        idx_df.select('_orig_index', 'text', 'clean_text_sample', 'text_sentiment').toPandas().to_csv(results_dir / 'sample_index_map.csv', index=False)\n",
    "    else:\n",
    "        idx_df.select('_orig_index', 'text', 'clean_text_sample', 'label').toPandas().to_csv(results_dir / 'sample_index_map.csv', index=False)\n",
    "\n",
    "    # Persist parquet outputs\n",
    "    processed_dir = Path(out_dir) / 'v1'\n",
    "    processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "    columns_to_save = ['text', 'text_sentiment', 'clean_text_sample', 'hashtag_count', 'mention_count', 'ticker_count', 'emoji_count']\n",
    "    columns_to_save += ['char_count', 'word_count', 'vader_compound', 'caps_ratio']\n",
    "    if 'lda_topics' in df.columns:\n",
    "        columns_to_save.append('lda_topics')\n",
    "\n",
    "    # Ensure present columns exist\n",
    "    columns_to_save = [c for c in columns_to_save if c in df.columns]\n",
    "\n",
    "    train_df.select(*columns_to_save).write.mode('overwrite').parquet(str(processed_dir / 'train.parquet'))\n",
    "    test_df.select(*columns_to_save).write.mode('overwrite').parquet(str(processed_dir / 'test.parquet'))\n",
    "\n",
    "    LOG.info('Saved train.parquet and test.parquet')\n",
    "\n",
    "# End run_preprocess_spark cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3e6a39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 19:08:53,214 INFO: Reading CSV from C:/Users/swkan/Downloads/VSCode/Big Data Group Project/data/stock_market_crash_2022.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset at: C:/Users/swkan/Downloads/VSCode/Big Data Group Project/data/stock_market_crash_2022.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 19:08:54,087 INFO: Dropped 12 duplicates\n",
      "2025-11-20 19:09:02,015 INFO: Saved train.parquet and test.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count: 1332\n",
      "Test count: 354\n",
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- text_sentiment: string (nullable = true)\n",
      " |-- clean_text_sample: string (nullable = true)\n",
      " |-- hashtag_count: integer (nullable = true)\n",
      " |-- mention_count: integer (nullable = true)\n",
      " |-- ticker_count: integer (nullable = true)\n",
      " |-- emoji_count: integer (nullable = true)\n",
      " |-- char_count: integer (nullable = true)\n",
      " |-- word_count: integer (nullable = true)\n",
      " |-- vader_compound: double (nullable = true)\n",
      " |-- caps_ratio: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell: Runner — set input & output and invoke the preprocessing function with path checks\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "# Default CSV path used in the script (relative to repo root)\n",
    "csv_path = 'data/stock_market_crash_2022.csv'  # use absolute override above if required\n",
    "out_dir = 'data/processed'\n",
    "\n",
    "# Helpful function to probe and find dataset; returns a path string that Spark accepts on Windows\n",
    "\n",
    "def find_dataset(path_like: str) -> str:\n",
    "    candidate = Path(path_like)\n",
    "\n",
    "    # Return absolute path string formatted with forward slashes (Windows-friendly)\n",
    "    def fmt(p: Path) -> str:\n",
    "        return p.resolve().as_posix()\n",
    "\n",
    "    if candidate.is_file():\n",
    "        return fmt(candidate)\n",
    "\n",
    "    filename = candidate.name\n",
    "    cwd = Path.cwd()\n",
    "    checked = []\n",
    "\n",
    "    # check common candidate locations\n",
    "    candidates = [cwd / path_like, cwd / filename, cwd.parent / path_like, cwd.parent / filename]\n",
    "\n",
    "    # walk parents and check for 'data/' + filename or direct file\n",
    "    for parent in [cwd] + list(cwd.parents):\n",
    "        candidates.append(parent / 'data' / filename)\n",
    "        candidates.append(parent / filename)\n",
    "\n",
    "    # include repo root guesses (1-3 levels up) as priorities\n",
    "    for parent in cwd.parents:\n",
    "        candidates.append(parent / 'data' / filename)\n",
    "\n",
    "    # include an explicit Windows-friendly location if the user provides one\n",
    "    # (this is the path the user supplied earlier; we prioritize it if present)\n",
    "    user_absolute_guess = Path(r\"C:\\Users\\swkan\\Downloads\\VSCode\\Big Data Group Project\\data\\stock_market_crash_2022.csv\")\n",
    "    candidates.insert(0, user_absolute_guess)\n",
    "\n",
    "    # unique candidates\n",
    "    seen = set()\n",
    "    for p in candidates:\n",
    "        if p in seen:\n",
    "            continue\n",
    "        seen.add(p)\n",
    "        checked.append(p)\n",
    "        if p.is_file():\n",
    "            return fmt(p)\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not find {filename}; checked paths:\\n\" + \"\\n\".join(map(str, checked)))\n",
    "\n",
    "# Resolve CSV path\n",
    "try:\n",
    "    csv_path_resolved = find_dataset(csv_path)\n",
    "    print('Using dataset at:', csv_path_resolved)\n",
    "except FileNotFoundError as e:\n",
    "    # print a helpful message and show current working dir for debugging\n",
    "    print(str(e))\n",
    "    print('Current working directory:', Path.cwd())\n",
    "    print('Try running from repo root or update csv_path to the dataset location.')\n",
    "    raise\n",
    "\n",
    "# Run preprocessing — this will create `data/processed/v1/train.parquet` and `test.parquet` and results files\n",
    "run_preprocess_spark(csv_path_resolved, out_dir, seed=42, test_size=0.2)\n",
    "\n",
    "# Quick validation — show counts\n",
    "from pyspark.sql.functions import count\n",
    "train = spark.read.parquet('data/processed/v1/train.parquet')\n",
    "test = spark.read.parquet('data/processed/v1/test.parquet')\n",
    "print('Train count:', train.count())\n",
    "print('Test count:', test.count())\n",
    "train.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd90e1",
   "metadata": {},
   "source": [
    "---\n",
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2986608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train/test sizes: 1332 354\n",
      "Creating numeric label from text_sentiment using StringIndexer\n",
      "Found existing trained pipeline at models\\pipelines\\baseline_logreg_default - loading\n",
      "Found existing trained pipeline at models\\pipelines\\baseline_logreg_default - loading\n",
      "Detected labels: [0, 1, 2]\n",
      "Detected labels: [0, 1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swkan\\Downloads\\VSCode\\Big Data Group Project\\.venv310\\lib\\site-packages\\pyspark\\sql\\context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metrics to results\\metrics\\baseline_logreg_default.json\n",
      "Saved confusion matrix to results\\metrics\\baseline_confusion_default.csv\n",
      "\n",
      "Summary:\n",
      " - Accuracy: 0.9153\n",
      " - Macro F1: 0.9157\n",
      " - Weighted F1: 0.9156\n",
      " - Labels: [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# Cell: Baseline A2 — TF-IDF + Logistic Regression (default params)\n",
    "# Trains a Spark ML Pipeline on the processed artifacts and saves model/metrics.\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Optional: set to True to force retrain even if saved pipeline exists\n",
    "force_retrain = False\n",
    "\n",
    "# Paths\n",
    "TRAIN_PARQUET = Path('data') / 'processed' / 'v1' / 'train.parquet'\n",
    "TEST_PARQUET = Path('data') / 'processed' / 'v1' / 'test.parquet'\n",
    "RESULTS_DIR = Path('results') / 'metrics'\n",
    "MODELS_DIR = Path('models') / 'pipelines'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PIPELINE_SAVE = str(MODELS_DIR / 'baseline_logreg_default')\n",
    "METRICS_SAVE_JSON = str(RESULTS_DIR / 'baseline_logreg_default.json')\n",
    "CONFUSION_SAVE_CSV = str(RESULTS_DIR / 'baseline_confusion_default.csv')\n",
    "\n",
    "# Load dataset\n",
    "target_train = spark.read.parquet(str(TRAIN_PARQUET))\n",
    "target_test = spark.read.parquet(str(TEST_PARQUET))\n",
    "\n",
    "print('Loaded train/test sizes:', target_train.count(), target_test.count())\n",
    "\n",
    "# Use local variables to avoid accidental reuse across runs\n",
    "train = target_train\n",
    "test = target_test\n",
    "\n",
    "# Create label column if missing: prefer 'label', else try 'text_sentiment'\n",
    "if 'label' not in train.columns:\n",
    "    if 'text_sentiment' in train.columns:\n",
    "        print('Creating numeric label from text_sentiment using StringIndexer')\n",
    "        idx = StringIndexer(inputCol='text_sentiment', outputCol='label', handleInvalid='keep').fit(train)\n",
    "        train = idx.transform(train)\n",
    "        test = idx.transform(test)\n",
    "        # Ensure 'label' is double\n",
    "        train = train.withColumn('label', train['label'].cast('double'))\n",
    "        test = test.withColumn('label', test['label'].cast('double'))\n",
    "    else:\n",
    "        raise RuntimeError('No label column found in train/test (expected label or text_sentiment)')\n",
    "\n",
    "# Minimal pipeline: Tokenize -> HashingTF -> IDF -> LogisticRegression\n",
    "tokenizer = Tokenizer(inputCol='clean_text_sample', outputCol='tokens')\n",
    "hashingTF = HashingTF(inputCol='tokens', outputCol='rawFeatures', numFeatures=10000)\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='tfidf')\n",
    "lr = LogisticRegression(featuresCol='tfidf', labelCol='label')\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, lr])\n",
    "\n",
    "# Guard: load or train based on force_retrain and existence\n",
    "if not force_retrain and Path(PIPELINE_SAVE).exists():\n",
    "    print('Found existing trained pipeline at', PIPELINE_SAVE, '- loading')\n",
    "    model = PipelineModel.load(PIPELINE_SAVE)\n",
    "else:\n",
    "    print('Training baseline pipeline (this may take a moment) ...')\n",
    "    model = pipeline.fit(train)\n",
    "    print('Model trained.')\n",
    "    try:\n",
    "        model.write().overwrite().save(PIPELINE_SAVE)\n",
    "        print('Saved pipeline to', PIPELINE_SAVE)\n",
    "    except Exception as e:\n",
    "        print('Warning: could not save pipeline to disk:', e)\n",
    "\n",
    "# Evaluate on test set\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Get label list\n",
    "labels = sorted([int(r['label']) for r in predictions.select('label').distinct().collect()])\n",
    "print('Detected labels:', labels)\n",
    "\n",
    "# Evaluate using MulticlassMetrics\n",
    "rdd_for_metrics = predictions.select('prediction', 'label').rdd.map(lambda r: (float(r['prediction']), float(r['label'])))\n",
    "metrics = MulticlassMetrics(rdd_for_metrics)\n",
    "\n",
    "# Macro/weighted/accuracy\n",
    "macro_f1 = float(np.mean([metrics.fMeasure(float(l)) for l in labels]))\n",
    "try:\n",
    "    weighted_f1 = float(metrics.weightedFMeasure())\n",
    "except Exception:\n",
    "    weighted_f1 = float(np.nan)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = float(evaluator.evaluate(predictions))\n",
    "\n",
    "confusion = metrics.confusionMatrix().toArray()\n",
    "confusion_df = pd.DataFrame(confusion, index=labels, columns=labels)\n",
    "\n",
    "# Save metrics\n",
    "metrics_out = {\n",
    "    'accuracy': accuracy,\n",
    "    'macro_f1': macro_f1,\n",
    "    'weighted_f1': weighted_f1,\n",
    "    'labels': labels,\n",
    "}\n",
    "with open(METRICS_SAVE_JSON, 'w', encoding='utf-8') as fh:\n",
    "    json.dump(metrics_out, fh, indent=2)\n",
    "confusion_df.to_csv(CONFUSION_SAVE_CSV)\n",
    "\n",
    "print('Saved metrics to', METRICS_SAVE_JSON)\n",
    "print('Saved confusion matrix to', CONFUSION_SAVE_CSV)\n",
    "\n",
    "# Short summary\n",
    "print('\\nSummary:')\n",
    "print(' - Accuracy:', round(accuracy, 4))\n",
    "print(' - Macro F1:', round(macro_f1, 4))\n",
    "print(' - Weighted F1:', round(weighted_f1, 4))\n",
    "print(' - Labels:', labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837829fc",
   "metadata": {},
   "source": [
    "---\n",
    "# Improved Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a01c3e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test rows: 1332 354\n",
      "Saved label indexer & mapping\n",
      "Found existing tuned pipeline: models\\pipelines\\baseline_logreg_tuned  — loading\n",
      "Saved label indexer & mapping\n",
      "Found existing tuned pipeline: models\\pipelines\\baseline_logreg_tuned  — loading\n",
      "Using `best_model` for evaluation; model loaded from disk? True\n",
      "Tuned best params: regParam= 0.01 elasticNetParam= 0.0\n",
      "Using `best_model` for evaluation; model loaded from disk? True\n",
      "Tuned best params: regParam= 0.01 elasticNetParam= 0.0\n",
      "\n",
      "Tuned Summary:\n",
      " - Accuracy: 0.9181\n",
      " - Macro F1: 0.9178\n",
      " - Weighted F1: 0.918\n",
      " - Labels: [0, 1, 2]\n",
      "\n",
      "Comparison (baseline vs tuned):\n",
      "Accuracy: 0.9153 -> 0.9181\n",
      "Macro F1: 0.9157 -> 0.9178\n",
      "Weighted F1: 0.9156 -> 0.918\n",
      "\n",
      "Tuned Summary:\n",
      " - Accuracy: 0.9181\n",
      " - Macro F1: 0.9178\n",
      " - Weighted F1: 0.918\n",
      " - Labels: [0, 1, 2]\n",
      "\n",
      "Comparison (baseline vs tuned):\n",
      "Accuracy: 0.9153 -> 0.9181\n",
      "Macro F1: 0.9157 -> 0.9178\n",
      "Weighted F1: 0.9156 -> 0.918\n"
     ]
    }
   ],
   "source": [
    "# Cell: A2.1 - Hyperparameter tuning (TrainValidationSplit) for Logistic Regression\n",
    "# Imports & safe fallbacks\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import time\n",
    "\n",
    "# Ensure directories/constants exist (if the baseline cell hasn't run)\n",
    "if 'RESULTS_DIR' not in globals():\n",
    "    RESULTS_DIR = Path('results') / 'metrics'\n",
    "if 'MODELS_DIR' not in globals():\n",
    "    MODELS_DIR = Path('models') / 'pipelines'\n",
    "if 'TRAIN_PARQUET' not in globals():\n",
    "    TRAIN_PARQUET = Path('data') / 'processed' / 'v1' / 'train.parquet'\n",
    "if 'TEST_PARQUET' not in globals():\n",
    "    TEST_PARQUET = Path('data') / 'processed' / 'v1' / 'test.parquet'\n",
    "\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Config\n",
    "TUNED_PIPELINE_SAVE = str(MODELS_DIR / 'baseline_logreg_tuned')\n",
    "TUNED_METRICS_JSON = str(RESULTS_DIR / 'baseline_logreg_tuned.json')\n",
    "TUNED_CONFUSION_CSV = str(RESULTS_DIR / 'baseline_confusion_tuned.csv')\n",
    "TUNED_PARAMS_JSON = str(RESULTS_DIR / 'baseline_logreg_tuned_params.json')\n",
    "\n",
    "force_retrain_tuned = False  # set True to force retrain\n",
    "use_sample_for_speed = False  # set True to sample train data while iterating\n",
    "sample_fraction = 0.2\n",
    "\n",
    "# Read train/test\n",
    "train = spark.read.parquet(str(TRAIN_PARQUET))\n",
    "test = spark.read.parquet(str(TEST_PARQUET))\n",
    "print('Train/test rows:', train.count(), test.count())\n",
    "\n",
    "# Ensure numeric label present (reuse code from baseline):\n",
    "if 'label' not in train.columns and 'text_sentiment' in train.columns:\n",
    "    idx = StringIndexer(inputCol='text_sentiment', outputCol='label', handleInvalid='keep').fit(train)\n",
    "    train = idx.transform(train)\n",
    "    test = idx.transform(test)\n",
    "    train = train.withColumn('label', train['label'].cast('double'))\n",
    "    test = test.withColumn('label', test['label'].cast('double'))\n",
    "    # Persist the fitted indexer & mapping for reproducibility\n",
    "    try:\n",
    "        label_indexer_save = str(MODELS_DIR / 'label_indexer')\n",
    "        idx.write().overwrite().save(label_indexer_save)\n",
    "        label_map = {lab: int(i) for i, lab in enumerate(idx.labels)} if hasattr(idx, 'labels') else {'labels': list(idx)}\n",
    "        with open(str(RESULTS_DIR.parent / 'label_indexer.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(label_map, f, indent=2)\n",
    "        print('Saved label indexer & mapping')\n",
    "    except Exception as e:\n",
    "        print('Warning: could not persist label indexer:', e)\n",
    "\n",
    "# Optionally sample train for faster grid search\n",
    "if use_sample_for_speed:\n",
    "    train_sample = train.sample(withReplacement=False, fraction=sample_fraction, seed=42)\n",
    "    print('Using train sample of', train_sample.count(), 'rows for tuning')\n",
    "else:\n",
    "    train_sample = train\n",
    "\n",
    "# Pipeline (same as baseline)\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='clean_text_sample', outputCol='tokens')\n",
    "hashingTF = HashingTF(inputCol='tokens', outputCol='rawFeatures', numFeatures=10000)\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='tfidf')\n",
    "lr = LogisticRegression(featuresCol='tfidf', labelCol='label')\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, lr])\n",
    "\n",
    "# Param grid\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0])\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .build()\n",
    "\n",
    "# Use weighted F1 as evaluator (f1 metric) for TrainValidationSplit\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1')\n",
    "\n",
    "# TrainValidationSplit (faster than CV)\n",
    "train_val = TrainValidationSplit(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, trainRatio=0.8, parallelism=2)\n",
    "\n",
    "# Fit or load tuned pipeline\n",
    "if not force_retrain_tuned and Path(TUNED_PIPELINE_SAVE).exists():\n",
    "    print('Found existing tuned pipeline:', TUNED_PIPELINE_SAVE, ' — loading')\n",
    "    tuned_model = PipelineModel.load(TUNED_PIPELINE_SAVE)\n",
    "    trained_from_disk = True\n",
    "else:\n",
    "    t0 = time.time()\n",
    "    print('Running TrainValidationSplit for tuning — this may take a while depending on parallelism and grid size...')\n",
    "    tuned_model = train_val.fit(train_sample)\n",
    "    trained_from_disk = False\n",
    "    print('Tuning completed in', round(time.time() - t0, 1), 'sec')\n",
    "    try:\n",
    "        tuned_model.bestModel.write().overwrite().save(TUNED_PIPELINE_SAVE)\n",
    "        print('Saved tuned best model at', TUNED_PIPELINE_SAVE)\n",
    "    except Exception as e:\n",
    "        print('Warning: could not persist tuned model:', e)\n",
    "\n",
    "# Extract the best pipeline model consistently (works if tuned_model is TrainValidationSplitModel or PipelineModel)\n",
    "best_model = tuned_model.bestModel if hasattr(tuned_model, 'bestModel') else tuned_model\n",
    "print('Using `best_model` for evaluation; model loaded from disk?' , trained_from_disk)\n",
    "\n",
    "# If available, print best param values from best_model\n",
    "try:\n",
    "    best_lr_stage = next(s for s in best_model.stages if type(s).__name__ == 'LogisticRegressionModel')\n",
    "    regParam_val = float(best_lr_stage._java_obj.getRegParam())\n",
    "    enet_val = float(best_lr_stage._java_obj.getElasticNetParam())\n",
    "    print('Tuned best params: regParam=', regParam_val, 'elasticNetParam=', enet_val)\n",
    "    # persist best param values to results\n",
    "    with open(TUNED_PARAMS_JSON, 'w', encoding='utf-8') as fh:\n",
    "        json.dump({'regParam': regParam_val, 'elasticNetParam': enet_val}, fh, indent=2)\n",
    "except Exception as e:\n",
    "    # not critical to fail; just print debug message\n",
    "    print('Could not read best params from the pipeline stages:', e)\n",
    "\n",
    "# Evaluate tuned model on test set\n",
    "predictions_tuned = best_model.transform(test)\n",
    "\n",
    "# Metrics (same approach as baseline)\n",
    "labels = sorted([int(r['label']) for r in predictions_tuned.select('label').distinct().collect()])\n",
    "metrics_rdd = predictions_tuned.select('prediction', 'label').rdd.map(lambda r: (float(r['prediction']), float(r['label'])))\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "metrics_m = MulticlassMetrics(metrics_rdd)\n",
    "macro_f1_tuned = float(np.mean([metrics_m.fMeasure(float(l)) for l in labels]))\n",
    "try:\n",
    "    weighted_f1_tuned = float(metrics_m.weightedFMeasure())\n",
    "except Exception:\n",
    "    weighted_f1_tuned = float(np.nan)\n",
    "accuracy_tuned = float(MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy').evaluate(predictions_tuned))\n",
    "confusion_df_tuned = pd.DataFrame(metrics_m.confusionMatrix().toArray(), index=labels, columns=labels)\n",
    "\n",
    "# Save tuned metrics\n",
    "metrics_tuned = {'accuracy': accuracy_tuned, 'macro_f1': macro_f1_tuned, 'weighted_f1': weighted_f1_tuned, 'labels': labels}\n",
    "with open(TUNED_METRICS_JSON, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics_tuned, f, indent=2)\n",
    "confusion_df_tuned.to_csv(TUNED_CONFUSION_CSV)\n",
    "\n",
    "print('\\nTuned Summary:')\n",
    "print(' - Accuracy:', round(accuracy_tuned, 4))\n",
    "print(' - Macro F1:', round(macro_f1_tuned, 4))\n",
    "print(' - Weighted F1:', round(weighted_f1_tuned, 4))\n",
    "print(' - Labels:', labels)\n",
    "\n",
    "# Persist a small note indicating whether this model came from tuning or disk\n",
    "with open(str(RESULTS_DIR / 'baseline_logreg_tuned_meta.json'), 'w', encoding='utf-8') as fh:\n",
    "    json.dump({'loaded_from_disk': bool(trained_from_disk), 'pipeline_path': TUNED_PIPELINE_SAVE}, fh, indent=2)\n",
    "\n",
    "# Compare baseline metrics (if they exist)\n",
    "if os.path.exists(METRICS_SAVE_JSON):\n",
    "    with open(METRICS_SAVE_JSON, 'r', encoding='utf-8') as f:\n",
    "        baseline_metrics = json.load(f)\n",
    "    print('\\nComparison (baseline vs tuned):')\n",
    "    print('Accuracy:', round(baseline_metrics.get('accuracy', float('nan')), 4), '->', round(accuracy_tuned, 4))\n",
    "    print('Macro F1:', round(baseline_metrics.get('macro_f1', float('nan')), 4), '->', round(macro_f1_tuned, 4))\n",
    "    print('Weighted F1:', round(baseline_metrics.get('weighted_f1', float('nan')), 4), '->', round(weighted_f1_tuned, 4))\n",
    "else:\n",
    "    print('\\nBaseline metrics not found; tuned metrics saved.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209022cc",
   "metadata": {},
   "source": [
    "---\n",
    "# SVM TF-IDF + LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59d8099d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: train/test rows: 1332 354\n",
      "Found existing SVM pipeline: models\\pipelines\\svm_tfidf_default - loading\n",
      "Found existing SVM pipeline: models\\pipelines\\svm_tfidf_default - loading\n",
      "\n",
      "SVM Summary:\n",
      " - Accuracy: 0.9181\n",
      " - Macro F1: 0.918\n",
      " - Weighted F1: 0.918\n",
      " - Labels: [0, 1, 2]\n",
      "\n",
      "Comparison (Baseline -> SVM):\n",
      "Accuracy: 0.9153 -> 0.9181\n",
      "Macro F1: 0.9157 -> 0.918\n",
      "Weighted F1: 0.9156 -> 0.918\n",
      "\n",
      "SVM Summary:\n",
      " - Accuracy: 0.9181\n",
      " - Macro F1: 0.918\n",
      " - Weighted F1: 0.918\n",
      " - Labels: [0, 1, 2]\n",
      "\n",
      "Comparison (Baseline -> SVM):\n",
      "Accuracy: 0.9153 -> 0.9181\n",
      "Macro F1: 0.9157 -> 0.918\n",
      "Weighted F1: 0.9156 -> 0.918\n"
     ]
    }
   ],
   "source": [
    "# Cell: A8 - SVM TF-IDF + LinearSVC (OneVsRest for multi-class)\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)\n",
    "\n",
    "# Harden imports and constants\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Paths & config\n",
    "RESULTS_DIR = Path('results') / 'metrics'\n",
    "MODELS_DIR = Path('models') / 'pipelines'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SVM_PIPELINE_SAVE = str(MODELS_DIR / 'svm_tfidf_default')\n",
    "SVM_METRICS_JSON = str(RESULTS_DIR / 'svm_tfidf_default.json')\n",
    "SVM_CONFUSION_CSV = str(RESULTS_DIR / 'svm_tfidf_confusion_default.csv')\n",
    "\n",
    "force_retrain_svm = False\n",
    "\n",
    "# Load train/test\n",
    "train = spark.read.parquet(str(TRAIN_PARQUET))\n",
    "test = spark.read.parquet(str(TEST_PARQUET))\n",
    "print('SVM: train/test rows:', train.count(), test.count())\n",
    "\n",
    "# Ensure numeric label present (reuse prior code if needed)\n",
    "if 'label' not in train.columns and 'text_sentiment' in train.columns:\n",
    "    from pyspark.ml.feature import StringIndexer\n",
    "    idx = StringIndexer(inputCol='text_sentiment', outputCol='label', handleInvalid='keep').fit(train)\n",
    "    train = idx.transform(train)\n",
    "    test = idx.transform(test)\n",
    "    train = train.withColumn('label', train['label'].cast('double'))\n",
    "    test = test.withColumn('label', test['label'].cast('double'))\n",
    "\n",
    "# Build pipeline (reuse tokenizer & hashingTF from earlier or re-create)\n",
    "# We choose same tokenization & hashingTF features as the baseline for fairness\n",
    "tokenizer = Tokenizer(inputCol='clean_text_sample', outputCol='tokens')\n",
    "hashingTF = HashingTF(inputCol='tokens', outputCol='rawFeatures', numFeatures=10000)\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='tfidf')\n",
    "\n",
    "# Choose LinearSVC as base estimator; for multiclass we use OneVsRest\n",
    "n_labels = len(train.select('label').distinct().collect())\n",
    "if n_labels > 2:\n",
    "    base_svc = LinearSVC(featuresCol='tfidf', labelCol='label', maxIter=100)\n",
    "    clf = OneVsRest(classifier=base_svc, labelCol='label', featuresCol='tfidf', predictionCol='prediction')\n",
    "else:\n",
    "    clf = LinearSVC(featuresCol='tfidf', labelCol='label', maxIter=100)\n",
    "\n",
    "pipeline_svm = Pipeline(stages=[tokenizer, hashingTF, idf, clf])\n",
    "\n",
    "# Train or load\n",
    "if not force_retrain_svm and Path(SVM_PIPELINE_SAVE).exists():\n",
    "    print('Found existing SVM pipeline:', SVM_PIPELINE_SAVE, '- loading')\n",
    "    svm_model = PipelineModel.load(SVM_PIPELINE_SAVE)\n",
    "else:\n",
    "    print('Training SVM pipeline...')\n",
    "    svm_model = pipeline_svm.fit(train)\n",
    "    try:\n",
    "        svm_model.write().overwrite().save(SVM_PIPELINE_SAVE)\n",
    "        print('Saved SVM pipeline to', SVM_PIPELINE_SAVE)\n",
    "    except Exception as e:\n",
    "        print('Warning: could not persist SVM pipeline:', e)\n",
    "\n",
    "# Evaluate\n",
    "predictions_svm = svm_model.transform(test)\n",
    "labels = sorted([int(r['label']) for r in predictions_svm.select('label').distinct().collect()])\n",
    "rdd_for_metrics = predictions_svm.select('prediction', 'label').rdd.map(lambda r: (float(r['prediction']), float(r['label'])))\n",
    "metrics = MulticlassMetrics(rdd_for_metrics)\n",
    "macro_f1 = float(np.mean([metrics.fMeasure(float(l)) for l in labels]))\n",
    "try:\n",
    "    weighted_f1 = float(metrics.weightedFMeasure())\n",
    "except Exception:\n",
    "    weighted_f1 = float(np.nan)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = float(evaluator.evaluate(predictions_svm))\n",
    "confusion = metrics.confusionMatrix().toArray()\n",
    "confusion_df = pd.DataFrame(confusion, index=labels, columns=labels)\n",
    "\n",
    "# Save SVM metrics\n",
    "metrics_out = {'accuracy': accuracy, 'macro_f1': macro_f1, 'weighted_f1': weighted_f1, 'labels': labels}\n",
    "with open(SVM_METRICS_JSON, 'w', encoding='utf-8') as fh:\n",
    "    json.dump(metrics_out, fh, indent=2)\n",
    "confusion_df.to_csv(SVM_CONFUSION_CSV)\n",
    "\n",
    "print('\\nSVM Summary:')\n",
    "print(' - Accuracy:', round(accuracy, 4))\n",
    "print(' - Macro F1:', round(macro_f1, 4))\n",
    "print(' - Weighted F1:', round(weighted_f1, 4))\n",
    "print(' - Labels:', labels)\n",
    "\n",
    "# Compare to baseline if baseline exists\n",
    "if os.path.exists(METRICS_SAVE_JSON):\n",
    "    with open(METRICS_SAVE_JSON, 'r', encoding='utf-8') as fh:\n",
    "        baseline_metrics = json.load(fh)\n",
    "    print('\\nComparison (Baseline -> SVM):')\n",
    "    print('Accuracy:', round(baseline_metrics.get('accuracy', float('nan')), 4), '->', round(accuracy, 4))\n",
    "    print('Macro F1:', round(baseline_metrics.get('macro_f1', float('nan')), 4), '->', round(macro_f1, 4))\n",
    "    print('Weighted F1:', round(baseline_metrics.get('weighted_f1', float('nan')), 4), '->', round(weighted_f1, 4))\n",
    "else:\n",
    "    print('\\nBaseline metrics not found; SVM metrics saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39b862d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER pipeline: train/test rows: 1332 354\n",
      "Using numeric features: ['char_count', 'word_count', 'hashtag_count', 'mention_count', 'ticker_count', 'caps_ratio', 'emoji_count', 'vader_compound']\n",
      "Found existing VADER pipeline: models\\pipelines\\baseline_vader_features_default - loading\n",
      "Using numeric features: ['char_count', 'word_count', 'hashtag_count', 'mention_count', 'ticker_count', 'caps_ratio', 'emoji_count', 'vader_compound']\n",
      "Found existing VADER pipeline: models\\pipelines\\baseline_vader_features_default - loading\n",
      "\n",
      "VADER-enhanced Summary:\n",
      " - Accuracy: 0.9294\n",
      " - Macro F1: 0.9289\n",
      " - Weighted F1: 0.9296\n",
      " - Labels: [0, 1, 2]\n",
      "\n",
      "Comparison (Baseline -> VADER-enhanced):\n",
      "Accuracy: 0.9153 -> 0.9294\n",
      "Macro F1: 0.9157 -> 0.9289\n",
      "Weighted F1: 0.9156 -> 0.9296\n",
      "\n",
      "VADER-enhanced Summary:\n",
      " - Accuracy: 0.9294\n",
      " - Macro F1: 0.9289\n",
      " - Weighted F1: 0.9296\n",
      " - Labels: [0, 1, 2]\n",
      "\n",
      "Comparison (Baseline -> VADER-enhanced):\n",
      "Accuracy: 0.9153 -> 0.9294\n",
      "Macro F1: 0.9157 -> 0.9289\n",
      "Weighted F1: 0.9156 -> 0.9296\n"
     ]
    }
   ],
   "source": [
    "# Cell: A12 - Add VADER / Lexicon features (TF-IDF + numeric features) for Logistic Regression\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Paths & config\n",
    "RESULTS_DIR = Path('results') / 'metrics'\n",
    "MODELS_DIR = Path('models') / 'pipelines'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "VADER_PIPELINE_SAVE = str(MODELS_DIR / 'baseline_vader_features_default')\n",
    "VADER_METRICS_JSON = str(RESULTS_DIR / 'baseline_vader_features_default.json')\n",
    "VADER_CONFUSION_CSV = str(RESULTS_DIR / 'baseline_vader_features_confusion_default.csv')\n",
    "\n",
    "force_retrain_vader = False\n",
    "\n",
    "# Load train/test\n",
    "train = spark.read.parquet(str(TRAIN_PARQUET))\n",
    "test = spark.read.parquet(str(TEST_PARQUET))\n",
    "print('VADER pipeline: train/test rows:', train.count(), test.count())\n",
    "\n",
    "# Ensure numeric label present (reuse prior code if needed)\n",
    "if 'label' not in train.columns and 'text_sentiment' in train.columns:\n",
    "    from pyspark.ml.feature import StringIndexer\n",
    "    idx = StringIndexer(inputCol='text_sentiment', outputCol='label', handleInvalid='keep').fit(train)\n",
    "    train = idx.transform(train)\n",
    "    test = idx.transform(test)\n",
    "    train = train.withColumn('label', train['label'].cast('double'))\n",
    "    test = test.withColumn('label', test['label'].cast('double'))\n",
    "\n",
    "# Identify candidate numeric lexicon features present in DF\n",
    "candidate_features = ['char_count','word_count','hashtag_count','mention_count','ticker_count','caps_ratio','emoji_count','vader_compound','vader_pos','vader_neu','vader_neg']\n",
    "present_features = [c for c in candidate_features if c in train.columns]\n",
    "print('Using numeric features:', present_features)\n",
    "\n",
    "# Build pipeline: Tokenizer -> HashingTF -> IDF\n",
    "tokenizer = Tokenizer(inputCol='clean_text_sample', outputCol='tokens')\n",
    "hashingTF = HashingTF(inputCol='tokens', outputCol='rawFeatures', numFeatures=10000)\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='tfidf')\n",
    "\n",
    "# VectorAssembler: combine TF-IDF vector with numeric features\n",
    "assembler_inputs = ['tfidf'] + present_features\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol='features')\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "pipeline_vader = Pipeline(stages=[tokenizer, hashingTF, idf, assembler, lr])\n",
    "\n",
    "# Guard: load or train\n",
    "if not force_retrain_vader and Path(VADER_PIPELINE_SAVE).exists():\n",
    "    print('Found existing VADER pipeline:', VADER_PIPELINE_SAVE, '- loading')\n",
    "    vader_model = PipelineModel.load(VADER_PIPELINE_SAVE)\n",
    "else:\n",
    "    print('Training VADER pipeline (TF-IDF + numeric features)...')\n",
    "    vader_model = pipeline_vader.fit(train)\n",
    "    try:\n",
    "        vader_model.write().overwrite().save(VADER_PIPELINE_SAVE)\n",
    "        print('Saved VADER pipeline to', VADER_PIPELINE_SAVE)\n",
    "    except Exception as e:\n",
    "        print('Warning: could not persist VADER pipeline:', e)\n",
    "\n",
    "# Evaluate\n",
    "preds_vader = vader_model.transform(test)\n",
    "labels = sorted([int(r['label']) for r in preds_vader.select('label').distinct().collect()])\n",
    "rdd_for_metrics = preds_vader.select('prediction', 'label').rdd.map(lambda r: (float(r['prediction']), float(r['label'])))\n",
    "metrics = MulticlassMetrics(rdd_for_metrics)\n",
    "macro_f1 = float(np.mean([metrics.fMeasure(float(l)) for l in labels]))\n",
    "try:\n",
    "    weighted_f1 = float(metrics.weightedFMeasure())\n",
    "except Exception:\n",
    "    weighted_f1 = float(np.nan)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = float(evaluator.evaluate(preds_vader))\n",
    "confusion = metrics.confusionMatrix().toArray()\n",
    "confusion_df = pd.DataFrame(confusion, index=labels, columns=labels)\n",
    "\n",
    "# Save metrics\n",
    "metrics_out = {'accuracy': accuracy, 'macro_f1': macro_f1, 'weighted_f1': weighted_f1, 'labels': labels, 'features_used': present_features}\n",
    "with open(VADER_METRICS_JSON, 'w', encoding='utf-8') as fh:\n",
    "    json.dump(metrics_out, fh, indent=2)\n",
    "confusion_df.to_csv(VADER_CONFUSION_CSV)\n",
    "\n",
    "print('\\nVADER-enhanced Summary:')\n",
    "print(' - Accuracy:', round(accuracy, 4))\n",
    "print(' - Macro F1:', round(macro_f1, 4))\n",
    "print(' - Weighted F1:', round(weighted_f1, 4))\n",
    "print(' - Labels:', labels)\n",
    "\n",
    "# Compare to baseline if baseline exists\n",
    "if os.path.exists(METRICS_SAVE_JSON):\n",
    "    with open(METRICS_SAVE_JSON, 'r', encoding='utf-8') as fh:\n",
    "        baseline_metrics = json.load(fh)\n",
    "    print('\\nComparison (Baseline -> VADER-enhanced):')\n",
    "    print('Accuracy:', round(baseline_metrics.get('accuracy', float('nan')), 4), '->', round(accuracy, 4))\n",
    "    print('Macro F1:', round(baseline_metrics.get('macro_f1', float('nan')), 4), '->', round(macro_f1, 4))\n",
    "    print('Weighted F1:', round(baseline_metrics.get('weighted_f1', float('nan')), 4), '->', round(weighted_f1, 4))\n",
    "else:\n",
    "    print('\\nBaseline metrics not found; VADER metrics saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd09b2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest: train/test rows: 1332 354\n",
      "Present numeric features for RF candidate: ['char_count', 'word_count', 'hashtag_count', 'mention_count', 'ticker_count', 'caps_ratio', 'emoji_count', 'vader_compound']\n",
      "Found existing RF pipeline: models\\pipelines\\randomforest_tfidf_default - loading\n",
      "Present numeric features for RF candidate: ['char_count', 'word_count', 'hashtag_count', 'mention_count', 'ticker_count', 'caps_ratio', 'emoji_count', 'vader_compound']\n",
      "Found existing RF pipeline: models\\pipelines\\randomforest_tfidf_default - loading\n",
      "Found existing tuned RF pipeline: models\\pipelines\\randomforest_tfidf_tuned - loading\n",
      "Found existing tuned RF pipeline: models\\pipelines\\randomforest_tfidf_tuned - loading\n",
      "\n",
      "RandomForest Summary (no tuning):\n",
      " - Accuracy: 0.5282\n",
      " - Macro F1: 0.4613\n",
      " - Weighted F1: 0.4779\n",
      " - Labels: [0, 1, 2]\n",
      "\n",
      "RandomForest Summary (no tuning):\n",
      " - Accuracy: 0.5282\n",
      " - Macro F1: 0.4613\n",
      " - Weighted F1: 0.4779\n",
      " - Labels: [0, 1, 2]\n",
      "\n",
      "RandomForest Tuned Summary:\n",
      " - Accuracy: 0.6356\n",
      " - Macro F1: 0.6078\n",
      " - Weighted F1: 0.6158\n",
      " - Labels: [0, 1, 2]\n",
      "\n",
      "Comparison (Baseline -> RF):\n",
      "Accuracy: 0.9153 -> 0.5282\n",
      "Macro F1: 0.9157 -> 0.4613\n",
      "Weighted F1: 0.9156 -> 0.4779\n",
      "\n",
      "Comparison (Baseline -> RF Tuned):\n",
      "Accuracy: 0.9153 -> 0.6356\n",
      "Macro F1: 0.9157 -> 0.6078\n",
      "Weighted F1: 0.9156 -> 0.6158\n",
      "Saved Baseline -> RF Tuned comparison to results\\metrics\\comparison_baseline_rf_tuned.json\n",
      "\n",
      "RandomForest Tuned Summary:\n",
      " - Accuracy: 0.6356\n",
      " - Macro F1: 0.6078\n",
      " - Weighted F1: 0.6158\n",
      " - Labels: [0, 1, 2]\n",
      "\n",
      "Comparison (Baseline -> RF):\n",
      "Accuracy: 0.9153 -> 0.5282\n",
      "Macro F1: 0.9157 -> 0.4613\n",
      "Weighted F1: 0.9156 -> 0.4779\n",
      "\n",
      "Comparison (Baseline -> RF Tuned):\n",
      "Accuracy: 0.9153 -> 0.6356\n",
      "Macro F1: 0.9157 -> 0.6078\n",
      "Weighted F1: 0.9156 -> 0.6158\n",
      "Saved Baseline -> RF Tuned comparison to results\\metrics\\comparison_baseline_rf_tuned.json\n"
     ]
    }
   ],
   "source": [
    "# Cell: A3 - RandomForest pipeline (TF-IDF [+ VADER features]) with CrossValidator tuning\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)\n",
    "\n",
    "# Imports & config\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Paths & constants\n",
    "if 'RESULTS_DIR' not in globals():\n",
    "    RESULTS_DIR = Path('results') / 'metrics'\n",
    "if 'MODELS_DIR' not in globals():\n",
    "    MODELS_DIR = Path('models') / 'pipelines'\n",
    "if 'TRAIN_PARQUET' not in globals():\n",
    "    TRAIN_PARQUET = Path('data') / 'processed' / 'v1' / 'train.parquet'\n",
    "if 'TEST_PARQUET' not in globals():\n",
    "    TEST_PARQUET = Path('data') / 'processed' / 'v1' / 'test.parquet'\n",
    "\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RF_PIPELINE_SAVE = str(MODELS_DIR / 'randomforest_tfidf_default')\n",
    "RF_TUNED_SAVE = str(MODELS_DIR / 'randomforest_tfidf_tuned')\n",
    "RF_METRICS_JSON = str(RESULTS_DIR / 'randomforest_tfidf_default.json')\n",
    "RF_TUNED_METRICS_JSON = str(RESULTS_DIR / 'randomforest_tfidf_tuned.json')\n",
    "RF_CONFUSION_CSV = str(RESULTS_DIR / 'randomforest_tfidf_confusion_default.csv')\n",
    "RF_TUNED_CONFUSION_CSV = str(RESULTS_DIR / 'randomforest_tfidf_confusion_tuned.csv')\n",
    "\n",
    "force_retrain_rf = False\n",
    "force_retrain_rf_tuned = False\n",
    "use_sample_for_speed = False\n",
    "sample_fraction = 0.2\n",
    "include_vader_features = False  # set True to include numeric VADER/emoji features in the pipeline\n",
    "\n",
    "# Load dataset\n",
    "train = spark.read.parquet(str(TRAIN_PARQUET))\n",
    "test = spark.read.parquet(str(TEST_PARQUET))\n",
    "print('RandomForest: train/test rows:', train.count(), test.count())\n",
    "\n",
    "# Ensure label column exists and is numeric\n",
    "if 'label' not in train.columns and 'text_sentiment' in train.columns:\n",
    "    from pyspark.ml.feature import StringIndexer\n",
    "    idx = StringIndexer(inputCol='text_sentiment', outputCol='label', handleInvalid='keep').fit(train)\n",
    "    train = idx.transform(train)\n",
    "    test = idx.transform(test)\n",
    "    train = train.withColumn('label', train['label'].cast('double'))\n",
    "    test = test.withColumn('label', test['label'].cast('double'))\n",
    "\n",
    "# Define feature list\n",
    "candidate_numeric = ['char_count','word_count','hashtag_count','mention_count','ticker_count','caps_ratio','emoji_count','vader_compound','vader_pos','vader_neu','vader_neg']\n",
    "present_numeric = [c for c in candidate_numeric if c in train.columns]\n",
    "print('Present numeric features for RF candidate:', present_numeric)\n",
    "\n",
    "# Build base pipeline steps\n",
    "tokenizer = Tokenizer(inputCol='clean_text_sample', outputCol='tokens')\n",
    "hashingTF = HashingTF(inputCol='tokens', outputCol='rawFeatures', numFeatures=10000)\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='tfidf')\n",
    "\n",
    "assembler_inputs = ['tfidf'] + (present_numeric if include_vader_features else [])\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol='features')\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='label', seed=42)\n",
    "\n",
    "pipeline_rf = Pipeline(stages=[tokenizer, hashingTF, idf, assembler, rf])\n",
    "\n",
    "# Optionally tune using CrossValidator (3-fold for speed)\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(rf.numTrees, [20, 50])\\\n",
    "    .addGrid(rf.maxDepth, [5, 10])\\\n",
    "    .build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1')\n",
    "cv = CrossValidator(estimator=pipeline_rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3, parallelism=2)\n",
    "\n",
    "# Optional sample for speed\n",
    "if use_sample_for_speed:\n",
    "    train_sample = train.sample(withReplacement=False, fraction=sample_fraction, seed=42)\n",
    "    print('Using sample for RF tuning: rows=', train_sample.count())\n",
    "else:\n",
    "    train_sample = train\n",
    "\n",
    "# Train or load un-tuned model\n",
    "if not force_retrain_rf and Path(RF_PIPELINE_SAVE).exists():\n",
    "    print('Found existing RF pipeline:', RF_PIPELINE_SAVE, '- loading')\n",
    "    rf_model = PipelineModel.load(RF_PIPELINE_SAVE)\n",
    "else:\n",
    "    print('Training RandomForest pipeline (no tuning)...')\n",
    "    rf_model = pipeline_rf.fit(train)\n",
    "    try:\n",
    "        rf_model.write().overwrite().save(RF_PIPELINE_SAVE)\n",
    "        print('Saved RF pipeline to', RF_PIPELINE_SAVE)\n",
    "    except Exception as e:\n",
    "        print('Warning: could not persist RF pipeline:', e)\n",
    "\n",
    "# Train tuned model or load tuned\n",
    "if not force_retrain_rf_tuned and Path(RF_TUNED_SAVE).exists():\n",
    "    print('Found existing tuned RF pipeline:', RF_TUNED_SAVE, '- loading')\n",
    "    rf_tuned = PipelineModel.load(RF_TUNED_SAVE)\n",
    "    trained_rf_tuned_from_disk = True\n",
    "else:\n",
    "    print('Running CrossValidator for RF tuning (this may take some time)...')\n",
    "    rf_cv_model = cv.fit(train_sample)\n",
    "    rf_tuned = rf_cv_model.bestModel\n",
    "    trained_rf_tuned_from_disk = False\n",
    "    try:\n",
    "        rf_tuned.write().overwrite().save(RF_TUNED_SAVE)\n",
    "        print('Saved tuned RF best model to', RF_TUNED_SAVE)\n",
    "    except Exception as e:\n",
    "        print('Warning: could not persist tuned RF model:', e)\n",
    "\n",
    "# Evaluate both models on test set\n",
    "preds_rf = rf_model.transform(test)\n",
    "labels = sorted([int(r['label']) for r in preds_rf.select('label').distinct().collect()])\n",
    "rdd_for_metrics = preds_rf.select('prediction', 'label').rdd.map(lambda r: (float(r['prediction']), float(r['label'])))\n",
    "metrics_rf = MulticlassMetrics(rdd_for_metrics)\n",
    "macro_f1_rf = float(np.mean([metrics_rf.fMeasure(float(l)) for l in labels]))\n",
    "try:\n",
    "    weighted_f1_rf = float(metrics_rf.weightedFMeasure())\n",
    "except Exception:\n",
    "    weighted_f1_rf = float(np.nan)\n",
    "accuracy_rf = float(MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy').evaluate(preds_rf))\n",
    "confusion_df_rf = pd.DataFrame(metrics_rf.confusionMatrix().toArray(), index=labels, columns=labels)\n",
    "\n",
    "# Save RF metrics\n",
    "metrics_out_rf = {'accuracy': accuracy_rf, 'macro_f1': macro_f1_rf, 'weighted_f1': weighted_f1_rf, 'labels': labels}\n",
    "with open(RF_METRICS_JSON, 'w', encoding='utf-8') as fh:\n",
    "    json.dump(metrics_out_rf, fh, indent=2)\n",
    "confusion_df_rf.to_csv(RF_CONFUSION_CSV)\n",
    "\n",
    "print('\\nRandomForest Summary (no tuning):')\n",
    "print(' - Accuracy:', round(accuracy_rf, 4))\n",
    "print(' - Macro F1:', round(macro_f1_rf, 4))\n",
    "print(' - Weighted F1:', round(weighted_f1_rf, 4))\n",
    "print(' - Labels:', labels)\n",
    "\n",
    "# Evaluate tuned RF if present\n",
    "preds_rf_tuned = rf_tuned.transform(test)\n",
    "labels_t = sorted([int(r['label']) for r in preds_rf_tuned.select('label').distinct().collect()])\n",
    "metrics_rdd_t = preds_rf_tuned.select('prediction', 'label').rdd.map(lambda r: (float(r['prediction']), float(r['label'])))\n",
    "metrics_rft = MulticlassMetrics(metrics_rdd_t)\n",
    "macro_f1_rft = float(np.mean([metrics_rft.fMeasure(float(l)) for l in labels_t]))\n",
    "try:\n",
    "    weighted_f1_rft = float(metrics_rft.weightedFMeasure())\n",
    "except Exception:\n",
    "    weighted_f1_rft = float(np.nan)\n",
    "accuracy_rft = float(MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy').evaluate(preds_rf_tuned))\n",
    "confusion_df_rft = pd.DataFrame(metrics_rft.confusionMatrix().toArray(), index=labels_t, columns=labels_t)\n",
    "\n",
    "# Save tuned RF metrics\n",
    "metrics_out_rft = {'accuracy': accuracy_rft, 'macro_f1': macro_f1_rft, 'weighted_f1': weighted_f1_rft, 'labels': labels_t}\n",
    "with open(RF_TUNED_METRICS_JSON, 'w', encoding='utf-8') as fh:\n",
    "    json.dump(metrics_out_rft, fh, indent=2)\n",
    "confusion_df_rft.to_csv(RF_TUNED_CONFUSION_CSV)\n",
    "\n",
    "print('\\nRandomForest Tuned Summary:')\n",
    "print(' - Accuracy:', round(accuracy_rft, 4))\n",
    "print(' - Macro F1:', round(macro_f1_rft, 4))\n",
    "print(' - Weighted F1:', round(weighted_f1_rft, 4))\n",
    "print(' - Labels:', labels_t)\n",
    "\n",
    "# Compare baseline metrics (un-tuned) to baseline metrics\n",
    
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcecb5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },"    print('\\nBaseline metrics not found; RF metrics saved.')\n",
    "\n",
    "# --- NEW: Compare Baseline vs RF Tuned and save JSON summary ---\n",
    "# Ensure baseline metrics path constant exists, fallback if not\n",
    "if 'METRICS_SAVE_JSON' not in globals():\n",
    "    METRICS_SAVE_JSON = str(Path('results') / 'metrics' / 'baseline_logreg_default.json')\n",
    "\n",
    "co, i am kovey ! mparison_path = Path(RESULTS_DIR) / 'comparison_baseline_rf_tuned.json'\n",
    "\n",
    "if os.path.exists(METRICS_SAVE_JSON) and os.path.exists(RF_TUNED_METRICS_JSON):\n",
    "    with open(METRICS_SAVE_JSON, 'r', encoding='utf-8') as f:\n",
    "        baseline_metrics = json.load(f)\n",
    "    with open(RF_TUNED_METRICS_JSON, 'r', encoding='utf-8') as f:\n",
    "        rf_tuned_metrics = json.load(f)\n",
    "\n",
    "    def compute_delta(a, b):\n",
    "        try:\n",
    "            return round(float(b) - float(a), 4)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    print('\\nComparison (Baseline -> RF Tuned):')\n",
    "    print('Accuracy:', round(baseline_metrics.get('accuracy', float('nan')), 4), '->', round(rf_tuned_metrics.get('accuracy', float('nan')), 4))\n",
    "    print('Macro F1:', round(baseline_metrics.get('macro_f1', float('nan')), 4), '->', round(rf_tuned_metrics.get('macro_f1', float('nan')), 4))\n",
    "    print('Weighted F1:', round(baseline_metrics.get('weighted_f1', float('nan')), 4), '->', round(rf_tuned_metrics.get('weighted_f1', float('nan')), 4))\n",
    "\n",
    "    # Persist comparison to JSON for records\n",
    "    comparison_summary = {\n",
    "        'baseline': baseline_metrics,\n",
    "        'rf_tuned': rf_tuned_metrics\n",
    "        \n",
    "    }\n",
    "    with open(comparison_path, 'w', encoding='utf-8') as fh:\n",
    "        json.dump(comparison_summary, fh, indent=2)\n",
    "    print('Saved Baseline -> RF Tuned comparison to', str(comparison_path))\n",
    "else:\n",
    "    print('\\nBaseline or RF tuned metrics not found; could not compute comparison for Baseline -> RF Tuned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3d700f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tp_baseline</th>\n",
       "      <th>precision_baseline</th>\n",
       "      <th>recall_baseline</th>\n",
       "      <th>f1_baseline</th>\n",
       "      <th>tp_rf_tuned</th>\n",
       "      <th>precision_rf_tuned</th>\n",
       "      <th>recall_rf_tuned</th>\n",
       "      <th>f1_rf_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  tp_baseline  precision_baseline  recall_baseline  f1_baseline  \\\n",
       "0      0          0.0                 0.0              0.0          0.0   \n",
       "1      1          0.0                 0.0              0.0          0.0   \n",
       "2      2          0.0                 0.0              0.0          0.0   \n",
       "\n",
       "   tp_rf_tuned  precision_rf_tuned  recall_rf_tuned  f1_rf_tuned  \n",
       "0          0.0                 0.0              0.0          0.0  \n",
       "1          0.0                 0.0              0.0          0.0  \n",
       "2          0.0                 0.0              0.0          0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved confusion comparison plot to results\\metrics\\plots\\confusion_baseline_vs_rf_tuned.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell: Compare — Per-class metrics + confusion heatmaps (Baseline vs RF Tuned)\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "# Attempt to ensure numpy is present and >= 1.23 since matplotlib requires this\n",
    "import importlib, subprocess, sys\n",
    "\n",
    "def _version_tuple(v):\n",
    "    try:\n",
    "        parts = v.split('.')\n",
    "        return tuple(int(p) for p in parts[:3])\n",
    "    except Exception:\n",
    "        return (0,0,0)\n",
    "# Check if numpy is importable and has a compliant version\n",
    "try:\n",
    "    import numpy as np\n",
    "    _np_ok = _version_tuple(np.__version__) >= (1,23,0)\n",
    "except Exception:\n",
    "    np = None\n",
    "    _np_ok = False\n",
    "if not _np_ok:\n",
    "    print(\"NumPy missing or older than 1.23.\\nDO NOT upgrade compiled libs in a running kernel — instead create a fresh environment or use the pinned `requirements.txt` / `environment.yml` and restart the kernel.\")\n",
    "    print('\\nRecommended options:')\n",
    "    print('  - conda create -n bigdata -f environment.yml  # or use conda install with pinned versions')\n",
    "    print('  - pip install -r requirements.txt in a fresh virtualenv and restart kernel')\n",
    "\n",
    "# Ensure plotting libs are available in the running kernel; if not, try to install them\n",
    "# Note: matplotlib/seaborn are allowed to be installed; they are pure-Python wheels on many platforms.\n",
    "def ensure_pkg(pkg_name):\n",
    "    try:\n",
    "        importlib.import_module(pkg_name)\n",
    "    except Exception:\n",
    "        print(f'Package {pkg_name} not found. Installing...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_name])\n",
    "for _pkg in ('matplotlib', 'seaborn'):\n",
    "    ensure_pkg(_pkg)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Paths\n",
    "RESULTS_DIR = Path('results') / 'metrics'\n",
    "BASELINE_CONF = RESULTS_DIR / 'baseline_confusion_default.csv'\n",
    "RF_TUNED_CONF = RESULTS_DIR / 'randomforest_tfidf_confusion_tuned.csv'\n",
    "BASELINE_METRICS = RESULTS_DIR / 'baseline_logreg_default.json'\n",
    "RF_TUNED_METRICS = RESULTS_DIR / 'randomforest_tfidf_tuned.json'\n",
    "OUT_SUMMARY = RESULTS_DIR / 'comparison_baseline_rf_tuned_class_metrics.json'\n",
    "PLOTS_DIR = RESULTS_DIR / 'plots'\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Helper: compute per-class precision/recall/f1 from confusion matrix DataFrame\n",
    "def per_class_metrics(conf_df: pd.DataFrame):\n",
    "    cm = conf_df.values.astype(float)\n",
    "    tp = np.diagonal(cm)\n",
    "    fp = cm.sum(axis=0) - tp\n",
    "    fn = cm.sum(axis=1) - tp\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        precision = np.divide(tp, tp + fp, out=np.zeros_like(tp), where=(tp+fp)!=0)\n",
    "        recall = np.divide(tp, tp + fn, out=np.zeros_like(tp), where=(tp+fn)!=0)\n",
    "        f1 = np.divide(2 * precision * recall, precision + recall, out=np.zeros_like(tp), where=(precision+recall)!=0)\n",
    "    classes = [int(c) if str(c).isdigit() else c for c in conf_df.columns]\n",
    "    return pd.DataFrame({\n",
    "        'class': classes,\n",
    "        'tp': tp.tolist(),\n",
    "        'precision': precision.tolist(),\n",
    "        'recall': recall.tolist(),\n",
    "        'f1': f1.tolist()\n",
    "    })\n",
    "\n",
    "# Load confusions\n",
    "if BASELINE_CONF.exists() and RF_TUNED_CONF.exists():\n",
    "    baseline_conf = pd.read_csv(BASELINE_CONF, index_col=0)\n",
    "    rf_conf = pd.read_csv(RF_TUNED_CONF, index_col=0)\n",
    "\n",
    "    # Ensure same label ordering: align both by union of labels\n",
    "    labels = sorted(set(map(str, baseline_conf.index.tolist())) | set(map(str, rf_conf.index.tolist())), key=lambda x: int(x) if x.isdigit() else x)\n",
    "    baseline_conf = baseline_conf.reindex(index=labels, columns=labels, fill_value=0)\n",
    "    rf_conf = rf_conf.reindex(index=labels, columns=labels, fill_value=0)\n",
    "\n",
    "    # Compute per-class metrics\n",
    "    baseline_per_class = per_class_metrics(baseline_conf)\n",
    "    rf_per_class = per_class_metrics(rf_conf)\n",
    "\n",
    "    # Merge to compare\n",
    "    compare_df = baseline_per_class.merge(rf_per_class, on='class', suffixes=('_baseline', '_rf_tuned'))\n",
    "\n",
    "    # Save comparison\n",
    "    comp_summary = {\n",
    "        'per_class': compare_df.to_dict(orient='records'),\n",
    "        'labels': labels\n",
    "    }\n",
    "    with open(OUT_SUMMARY, 'w', encoding='utf-8') as fh:\n",
    "        json.dump(comp_summary, fh, indent=2)\n",
    "\n",
    "    # Print a short tabular view\n",
    "    display(compare_df)\n",
    "\n",
    "    # Plot confusion heatmaps side-by-side\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(baseline_conf.astype(int), annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Baseline Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(rf_conf.astype(int), annot=True, fmt='d', cmap='Oranges')\n",
    "    plt.title('RandomForest Tuned Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "\n",
    "    plot_file = PLOTS_DIR / 'confusion_baseline_vs_rf_tuned.png'\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "    print('Saved confusion comparison plot to', str(plot_file))\n",
    "\n",
    "else:\n",
    "    print('Baseline or RF tuned confusion CSVs not found; please run baseline and RF cells first.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ae954",
   "metadata": {},
   "source": [
    "hi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310 (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
