# Core runtimes (Python 3.10+)
# Use PySpark 3.5.x which supports Python 3.10+ and pandas-on-spark improvements
# Core runtime
pyspark==3.5.7
# Use newer numpy/pandas to avoid ABI mismatch with compiled extensions
# pinned compiled libs to avoid ABI mismatches
numpy==1.24.4
pandas==2.1.3
# We prefer PySpark-only flow; scikit-learn is optional (not required for A1/A2 baseline)
# scikit-learn==1.0.2

# Common ML / NLP libs - install via pip or environment.yml
sentence-transformers==2.2.2
vaderSentiment==3.3.2
gensim==4.3.1
# fasttext==0.9.2  # Optional: building on Windows may fail; install via conda-forge or native wheel if needed
# pin a pyarrow compatible with Spark >=3.4
pyarrow==11.0.0

# Pin jars and libraries that must be compatible with pyspark
# Py4J should match the one pyspark expects; pyspark 3.5.7 uses py4j==0.10.9.7
py4j==0.10.9.7

# Optional ML/plotting libs updated to the versions detected in the venv
# (update to these versions if you'd like to reuse the current venv setup)
scikit-learn==1.6.1
matplotlib==3.10.7
seaborn==0.13.2

# Note: For GPU-enabled PyTorch, install the appropriate wheel with CUDA support via conda or the
# official PyTorch instructions to match your GPU driver/cuda version.
# For example: conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia
